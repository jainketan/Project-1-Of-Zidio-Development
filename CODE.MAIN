# Importing required libraries
import pandas as pd
import numpy as np
import os
import seaborn as sns
import matplotlib.pyplot as plt
import librosa
import librosa.display
from IPython.display import Audio
import warnings
warnings.filterwarnings('ignore')

#!pip install kaggle

!mkdir ~/.kaggle/

!kaggle datasets download -d ejlok1/toronto-emotional-speech-set-test

!unzip toronto-emotional-speech-set-tess.zip

paths = []
labels = []

# Traverse through the dataset folder and extract paths and labels
for dirname, _, filenames in os.walk("/content/tess toronto emotional speech set data"):
    for filename in filenames:
        paths.append(os.path.join(dirname, filename))

        # Assuming the label is based on the last part of the filename before the extension
        label = filename.split('_')[-1]
        label = label.split('.')[0]
        labels.append(label.lower())

    # Break if the paths reach a specific number (if you want to limit the dataset)
    if len(paths) == 2800:
        break

print('Dataset is Loaded')

len(paths)

paths[:5]

labels[:5]

## Create a dataframe
df = pd.DataFrame()
df['speech'] = paths
df['label'] = labels
df.head() # Display the first few rows of the dataframe

# Count the distribution of each label
df['label'].value_counts()

# Display of the unique emotions in the label column
df['label'].unique()

# Plot the distribution of labels
sns.countplot(data=df, x='label')

# Function to plot waveform
def waveplot(data, sr, emotion):
    plt.figure(figsize=(10,4))
    plt.title(emotion, size=20)
    librosa.display.waveshow(data, sr=sr)
    plt.show()

# Function to plot spectogram
def spectogram(data, sr, emotion):
    x = librosa.stft(data)
    xdb = librosa.amplitude_to_db(abs(x))
    plt.figure(figsize=(11,4))
    plt.title(emotion, size=20)
    librosa.display.specshow(xdb, sr=sr, x_axis='time', y_axis='hz')
    plt.colorbar()

# Example usage for EDA visualization for each emotion
emotion = 'fear'
path = np.array(df['speech'][df['label']==emotion])[0]
data, sampling_rate = librosa.load(path)
waveplot(data, sampling_rate, emotion)
spectogram(data, sampling_rate, emotion)
Audio(path)

emotion = 'angry'
path = np.array(df['speech'][df['label']==emotion])[1]
data, sampling_rate = librosa.load(path)
waveplot(data, sampling_rate, emotion)
spectogram(data, sampling_rate, emotion)
Audio(path)

emotion = 'disgust'
path = np.array(df['speech'][df['label']==emotion])[0]
data, sampling_rate = librosa.load(path)
waveplot(data, sampling_rate, emotion)
spectogram(data, sampling_rate, emotion)
Audio(path)

emotion = 'neutral'
path = np.array(df['speech'][df['label']==emotion])[0]
data, sampling_rate = librosa.load(path)
waveplot(data, sampling_rate, emotion)
spectogram(data, sampling_rate, emotion)
Audio(path)

emotion = 'sad'
path = np.array(df['speech'][df['label']==emotion])[0]
data, sampling_rate = librosa.load(path)
waveplot(data, sampling_rate, emotion)
spectogram(data, sampling_rate, emotion)
Audio(path)

emotion = 'ps'
path = np.array(df['speech'][df['label']==emotion])[0]
data, sampling_rate = librosa.load(path)
waveplot(data, sampling_rate, emotion)
spectogram(data, sampling_rate, emotion)
Audio(path)

emotion = 'happy'
path = np.array(df['speech'][df['label']==emotion])[0]
data, sampling_rate = librosa.load(path)
waveplot(data, sampling_rate, emotion)
spectogram(data, sampling_rate, emotion)
Audio(path)

#Feature Extraction: Extract MFCC features from an audio file
def extract_mfcc(filename):
    y, sr = librosa.load(filename, duration=3, offset=0.5)
    mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)
    return mfcc

extract_mfcc(df['speech'][0])

X_mfcc = df['speech'].apply(lambda x: extract_mfcc(x))

X = [x for x in X_mfcc]
X = np.array(X)
X.shape

     from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder()
y = enc.fit_transform(df[['label']])

     y = y.toarray()

     y.shape

     # Splitting the data into training and validation sets
from sklearn.model_selection import train_test_split
X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.2,random_state=42)

     from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout

model = Sequential([
    LSTM(256, return_sequences=False, input_shape=(40,1)),
    Dropout(0.5),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(7, activation='softmax')
])

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

     # Train the model
history = model.fit(X, y, validation_data=(X_val,y_val), epochs=30, batch_size=64)
    [ Epoch 1/30
44/44 ━━━━━━━━━━━━━━━━━━━━ 12s 203ms/step - accuracy: 0.2657 - loss: 1.8105 - val_accuracy: 0.7196 - val_loss: 0.8638
Epoch 2/30
44/44 ━━━━━━━━━━━━━━━━━━━━ 7s 162ms/step - accuracy: 0.6108 - loss: 0.9943 - val_accuracy: 0.8196 - val_loss: 0.5078
Epoch 3/30
44/44 ━━━━━━━━━━━━━━━━━━━━ 12s 193ms/step - accuracy: 0.7429 - loss: 0.7048 - val_accuracy: 0.9339 - val_loss: 0.2384
Epoch 4/30
44/44 ━━━━━━━━━━━━━━━━━━━━ 9s 202ms/step - accuracy: 0.8276 - loss: 0.4621 - val_accuracy: 0.9464 - val_loss: 0.1895
Epoch 5/30
44/44 ━━━━━━━━━━━━━━━━━━━━ 9s 168ms/step - accuracy: 0.9074 - loss: 0.3005 - val_accuracy: 0.9304 - val_loss: 0.1969
Epoch 6/30
44/44 ━━━━━━━━━━━━━━━━━━━━ 10s 165ms/step - accuracy: 0.9203 - loss: 0.3012 - val_accuracy: 0.8911 - val_loss: 0.3060
Epoch 7/30
44/44 ━━━━━━━━━━━━━━━━━━━━ 12s 204ms/step - accuracy: 0.9032 - loss: 0.3834 - val_accuracy: 0.9768 - val_loss: 0.0704
Epoch 8/30
44/44 ━━━━━━━━━━━━━━━━━━━━ 10s 206ms/step - accuracy: 0.9502 - loss: 0.1921 - val_accuracy: 0.9643 - val_loss: 0.0936
Epoch 9/30
44/44 ━━━━━━━━━━━━━━━━━━━━ 8s 163ms/step - accuracy: 0.9426 - loss: 0.1971 - val_accuracy: 0.9929 - val_loss: 0.0208
Epoch 10/30
44/44 ━━━━━━━━━━━━━━━━━━━━ 11s 175ms/step - accuracy: 0.9580 - loss: 0.1671 - val_accuracy: 0.9857 - val_loss: 0.0432
Epoch 11/30
44/44 ━━━━━━━━━━━━━━━━━━━━ 12s 208ms/step - accuracy: 0.9595 - loss: 0.1510 - val_accuracy: 0.9768 - val_loss: 0.0854
Epoch 12/30
44/44 ━━━━━━━━━━━━━━━━━━━━ 10s 198ms/step - accuracy: 0.9466 - loss: 0.2184 - val_accuracy: 0.9911 - val_loss: 0.0442
Epoch 13/30
...
Epoch 29/30
44/44 ━━━━━━━━━━━━━━━━━━━━ 9s 167ms/step - accuracy: 0.9920 - loss: 0.0360 - val_accuracy: 0.9964 - val_loss: 0.0088
Epoch 30/30
44/44 ━━━━━━━━━━━━━━━━━━━━ 9s 199ms/step - accuracy: 0.9903 - loss: 0.0273 - val_accuracy: 0.9946 - val_loss: 0.0138]


     epochs = list(range(30))
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
plt.plot(epochs, acc, label='train accuracy')
plt.plot(epochs, val_acc, label='val accuracy')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()
plt.show()

     loss = history.history['loss']
val_loss = history.history['val_loss']
plt.plot(epochs, loss, label='train loss')
plt.plot(epochs, val_loss, label='val loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()
plt.show()
